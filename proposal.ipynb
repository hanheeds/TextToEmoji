{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGMAhIAC1k6i"
   },
   "source": [
    "# CS 449 Final Project Proposal\n",
    "\n",
    "Due: April 21, 2023 at 11:59pm\n",
    "\n",
    "## 1. Names and Net IDs\n",
    "\n",
    "Si Woo Park (swp1316) & Hanhee Yang (hym6769)\n",
    "\n",
    "## 2. Abstract\n",
    "\n",
    "*Your abstract should be two or three sentences describing the motivation\n",
    "for your project and your proposed methods.*\n",
    "\n",
    "In this project, we propose to develop a text classifier that identifies emotions in Reddit comments using the BERT transformer model. We will curate a dataset of comments with specific emotions and train our model on this data to achieve accurate classification. Ultimately, we aim to create a program that outputs an appropriate emoji based on the classified emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX2wBFIs2S9Q"
   },
   "source": [
    "## 3. Introduction\n",
    "\n",
    "*Why is this project interesting to you? Describe the motivation for pursuing this project. Give a specific description of your data and what machine learning task you will focus on.*\n",
    "\n",
    "Through this project, we will be able to work with the transformer model which has been a hot topic in the field of machine learning with the rise of ChatGPT. Emojis have become an important element of human communication, especially in conveying emotion without the use of voice and tone. Therefore, this model that accurately evaluates the emotion of a given text and suggests an appropriate emoji will be a useful and interesting tool that can aid people’s online communication. Our dataset has a compiled list of reddit comments along with a label that indicates the emotion that corresponds to the test. The machine learning task we will be focusing on is multi-label emotion classification using the BERT transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-1Lwrn635Qa"
   },
   "source": [
    "## 4a. Describe your dataset(s)\n",
    "\n",
    "*List the datasets you plan to use, where you found them, and what they contain. Be detailed! For each dataset, what does the data look like? What is the data representation? (e.g., what resolution of images? what length of sequences?) How is the data annotated or labeled? Include citations for the datasets. Include at least one citation of previous work that has used your data, or explain why no one has used your data before.*\n",
    "\n",
    "We decided to pick the GoEmotions dataset that contains 58,000 carefully curated Reddit comments labeled for 27 emotion different categories or Neutral. The dataset was created in 2020 by Amazon Alexa, Google Research, and Stanford. The dataset contains columns with, most importantly, the Reddit comment, id, clarity of the comment, and 27 different binary emotion columns, set to 1 if the specific emotion is present in the comment and 0 if not. The dataset is not presplit into train/test sets. Thus, our plan is to split the dataset into a 70-30 split for train/test splits respectively. \n",
    "\n",
    "One previous work that was done on the GoEmotions dataset was the paper, GoEmotions: A Dataset of Fine-Grained Emotions, which uses a BERT-based model to achieve an F-1 score of .46 for emotion prediction.\n",
    "\n",
    "Another dataset we decided to use was the EmoTag1200 dataset. This dataset basically contains Baseline Emoji Emotion Scores. This includes 1200 Emoji-Emotion pairs annotated by humans. It contains emotion scores ranging from 0 to 1 for 150 most popular Twitter emojis for 8 emotion classes (i.e. anger, anticipation, disgust, fear, joy, sadness, surprise, and trust).\n",
    "\n",
    "This dataset has been used before to analyze an emotion-based anlaysis of emojis. (Shoeb, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Load your dataset(s)\n",
    "\n",
    "*Demonstrate that you have made at least some progress with getting your\n",
    "dataset ready to use. Load at least a few examples and visualize them\n",
    "as best you can*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "VUfetuVm5WTy",
    "outputId": "30b7a0f8-9bb8-4ad9-9376-55fed29f23dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset go_emotions (/Users/siwoopark/.cache/huggingface/datasets/go_emotions/raw/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491154ccf56d47d789902fe342984476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: That game hurt.\n",
      "emotion: sadness \n",
      "\n",
      "text:  >sexuality shouldn’t be a grouping category It makes you different from othet ppl so imo it fits the definition of \"grouping\" \n",
      "emotion: \n",
      "\n",
      "text: You do right, if you don't care then fuck 'em!\n",
      "emotion: neutral \n",
      "\n",
      "text: Man I love reddit.\n",
      "emotion: love \n",
      "\n",
      "text: [NAME] was nowhere near them, he was by the Falcon. \n",
      "emotion: neutral \n",
      "\n",
      "text: Right? Considering it’s such an important document, I should know the damned thing backwards and forwards... thanks again for the help!\n",
      "emotion: gratitude \n",
      "\n",
      "text: He isn't as big, but he's still quite popular. I've heard the same thing about his content. Never watched him much.\n",
      "emotion: disapproval \n",
      "\n",
      "text: That's crazy; I went to a super [RELIGION] high school and I think I can remember 2 girls the entire 4 years that became teen moms.\n",
      "emotion: amusement \n",
      "\n",
      "text: that's adorable asf\n",
      "emotion: amusement \n",
      "\n",
      "text: \"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" finale is too real\n",
      "emotion: amusement \n",
      "\n",
      "text: I have, and now that you mention it, I think that's what triggered my nostalgia. \n",
      "emotion: neutral \n",
      "\n",
      "text: I wanted to downvote this, but it's not your fault homie.\n",
      "emotion: disappointment \n",
      "\n",
      "text: BUT IT'S HER TURN! /s\n",
      "emotion: neutral \n",
      "\n",
      "text: That is odd.\n",
      "emotion: disappointment disgust \n",
      "\n",
      "text: Build a wall? /jk\n",
      "emotion: neutral \n",
      "\n",
      "text: I appreciate it, that's good to know. I hope I'll have to apply that knowledge one day\n",
      "emotion: admiration gratitude \n",
      "\n",
      "text: One time my 1 stopped right in 91st, I was able to get a good photo of the platform since they have some lights along it.\n",
      "emotion: neutral \n",
      "\n",
      "text: Well then I’d say you have a pretty good chance if it’s any girl lol\n",
      "emotion: realization \n",
      "\n",
      "text: Pretty much every Punjabi dude I've met.\n",
      "emotion: admiration \n",
      "\n",
      "text: For extra measure tape it right by your crotch so she can't take it for sexual assault reasons\n",
      "emotion: annoyance \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\", \"raw\")\n",
    "emotions = [\n",
    "    \"admiration\",\n",
    "    \"amusement\",\n",
    "    \"anger\",\n",
    "    \"annoyance\",\n",
    "    \"approval\",\n",
    "    \"caring\",\n",
    "    \"confusion\",\n",
    "    \"curiosity\",\n",
    "    \"desire\",\n",
    "    \"disappointment\",\n",
    "    \"disapproval\",\n",
    "    \"disgust\",\n",
    "    \"embarrassment\",\n",
    "    \"excitement\",\n",
    "    \"fear\",\n",
    "    \"gratitude\",\n",
    "    \"grief\",\n",
    "    \"joy\",\n",
    "    \"love\",\n",
    "    \"nervousness\",\n",
    "    \"optimism\",\n",
    "    \"pride\",\n",
    "    \"realization\",\n",
    "    \"relief\",\n",
    "    \"remorse\",\n",
    "    \"sadness\",\n",
    "    'surprise',\n",
    "    \"neutral\",\n",
    "]\n",
    "\n",
    "i = 0\n",
    "while i < 20:\n",
    "    row = dataset['train'][i]\n",
    "    print('text:',row['text'])\n",
    "    print('emotion:',end=' ')\n",
    "    for emotion in emotions:\n",
    "        if row[emotion] == 1:\n",
    "            print(emotion,end=' ')\n",
    "    print('\\n')\n",
    "    i+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unicode  anger  anger-stddev  anticipation  anticipation-stddev  disgust  \\\n",
      "0   1F308   0.00          0.00          0.28                 0.42     0.00   \n",
      "1   1F319   0.00          0.00          0.31                 0.44     0.00   \n",
      "2   1F31A   0.06          0.16          0.08                 0.17     0.17   \n",
      "3   1F31E   0.00          0.00          0.22                 0.34     0.00   \n",
      "4   1F31F   0.00          0.00          0.28                 0.36     0.00   \n",
      "\n",
      "   disgust-stddev  fear  fear-stddev   joy  ...  sadness  sadness-stddev  \\\n",
      "0            0.00  0.00         0.00  0.69  ...     0.06            0.16   \n",
      "1            0.00  0.00         0.00  0.25  ...     0.00            0.00   \n",
      "2            0.24  0.06         0.16  0.42  ...     0.19            0.28   \n",
      "3            0.00  0.00         0.00  0.78  ...     0.00            0.00   \n",
      "4            0.00  0.00         0.00  0.53  ...     0.00            0.00   \n",
      "\n",
      "   surprise  surprise-stddev  trust  trust-stddev  emoji           name  \\\n",
      "0      0.22             0.36   0.33          0.47      🌈        rainbow   \n",
      "1      0.06             0.16   0.25          0.37      🌙  crescent moon   \n",
      "2      0.06             0.16   0.11          0.17      🌚  new moon face   \n",
      "3      0.11             0.21   0.22          0.34      🌞  sun with face   \n",
      "4      0.25             0.41   0.31          0.40      🌟   glowing star   \n",
      "\n",
      "             group      sub-group  \n",
      "0  Travel & Places  sky & weather  \n",
      "1  Travel & Places  sky & weather  \n",
      "2  Travel & Places  sky & weather  \n",
      "3  Travel & Places  sky & weather  \n",
      "4  Travel & Places  sky & weather  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "url = 'https://raw.githubusercontent.com/abushoeb/EmoTag/37d27757d90965d9a9b8a4ff712fe6533e9bb544/data/EmoTag1200-scores-details.csv?plain=1'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Print the first five rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Small dataset\n",
    "\n",
    "*Many deep learning datasets are very large, which is helpful for training powerful models but makes debugging difficult. For your update, you will need to construct a small version of your dataset that contains 200-1000 examples and is less than 10MB. If you are working with images, video, or audio, you may need to downsample your data. If you are working with text, you may need to truncate or otherwise preprocess your data.*\n",
    "\n",
    "*Give a specific plan for how you will create a small version of one dataset you'll use that is less than 10MB in size. Mention the current size of your dataset and how many examples it has and how those numbers inform your plan.*\n",
    "\n",
    "*Dataset #1: \n",
    "The current size of our main dataset is 47.1MB and it contains a total of 211k rows. We can use the first 30k rows in the small version of our dataset.\n",
    "\n",
    "*Dataset #2:\n",
    "The current size of the main dataset is 19.5MB and it contains a total of 245k rows. We will half the size of the predetermined train, validation, and test sets so that the total number of rows will be less than 125k rows, which will be less than 10MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCKtudoJ6V4g"
   },
   "source": [
    "## 5. Methods\n",
    "\n",
    "*Describe what methods you plan to use. This is a deep learning class, so you should use deep learning methods. Cite at least one or two relevant papers. What model architectures or pretrained models will you use? What loss function(s) will you use and why? How will you evaluate or visualize your model's performance?*\n",
    "\n",
    "For this multi-label classification task, we will employ deep learning methods, specifically utilizing a pretrained transformer model called BERT (Devlin, 2018). To fine-tune and train this model, we will preprocess the data by tokenizing the text using the AutoTokenizer API. The labels will be attached to the token encodings, resulting in a matrix of shape (batch_size, num_labels). (Demszky, 2020)\n",
    "\n",
    "During the training process, we will utilize the Trainer object, which incorporates hyperparameters, the pretrained model, training and evaluation datasets, and the tokenizer (https://huggingface.co/docs/transformers/main_classes/trainer#id1). Our model will be based on the AutoModelForSequenceClassification architecture, and we will fine-tune it with our dataset.\n",
    "\n",
    "To address the multi-label nature of the task, we will employ a Binary Cross Entropy loss for each of the 27 different emotions, combining them using a weighted average. The hyperparameters that can be adjusted include batch size, learning rate, weight decay, and the number of training epochs.\n",
    "\n",
    "First we will train the BERT model on the preprocessed text and emotion dataset. We will use libraries such as Hugging Face's Transformers or TensorFlow's Keras to train the model. The goal of pretraining is to teach the BERT model how to understand the language and the context of the text.\n",
    "\n",
    "Once we have predicted the emotions for a given text, we can use the emotion to emoji mappings to convert the predicted emotions into emojis. If the emotion and emoji dataset has a smaller set of emotions than the text and emotion dataset, we will need to create a mapping between the two emotion sets. One way to do this is to use clustering techniques to group similar emotions together, and then map each group to a corresponding emoji.\n",
    "\n",
    "To evaluate our model's performance, we will apply it to the test dataset and compare the predicted labels with the true labels. We will calculate classification accuracy, f1 score, and ROC AUC metrics and compare them to the performance of a baseline logistic regression model. The logistic regression model will serve as a benchmark, as it is easy to train, and we expect the neural model to outperform it.\n",
    "\n",
    "To visualize the training progress, we will plot the loss values over multiple epochs, providing a visual representation of the loss's evolution throughout the training process. This will aid in understanding the convergence and optimization of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA5eIKsR7QRk"
   },
   "source": [
    "## 6. Deliverables\n",
    "\n",
    "*Include at least six goals that you would like to focus on over the course of the quarter. These should be nontrivial, but you should have at least one and hopefully both of your \"Essential\" goals done by the project update, due in mid-May. Your \"Stretch\" goals should be ambitious enough such that completing one is doable, but completing both this quarter is unlikely.*\n",
    "\n",
    "### 6.1 Essential Goals\n",
    "- (At least two goals here. At least one should involve getting a neural network model running.)\n",
    "\n",
    "1. We want to be able to import and successfully run the transformer model by fine-tuning the neural model and be able to output any emojis. \n",
    "\n",
    "2. We want to preprocess our dataset into a format that our transformer model can understand and get trained on.\n",
    "\n",
    "### 6.2 Desired Goals\n",
    "- (At least two goals here. Completing these goals should be sufficient for you to say your project was a success.)\n",
    "\n",
    "1. One goal is to be able to fine tune and train the transformer model with our dataset to create a model that can predict correct emojis that align with actual labels.\n",
    "\n",
    "2. To be significantly better than baseline accuracy. A simple baseline to compare the accuracy to would be the logistic regression. Logistic regression will most likely perform poorly on this dataset beause predictions are based on a linear function. That being said by sweep tuning parameters such as learning rate, number of epochs, batch size, dropout rate, optimizers, and fine-tuning layers will be able to improve accuracy. To avoid overfitting, techniques such as dropout, L1, or L2 regularization can be applied to BERT as well as reducing the number of hidden units in a layer or using cross-validation can be used. \n",
    "\n",
    "3. Our goal is to develop a model that can map from text to emotion labels and then use the embeddings of emojis to map from emotion labels to corresponding emojis. Once the BERT model is trained to predict emotions, the predicted emotion label will be used to identify the appropriate emoji. To achieve this, we will use the embeddings of emojis, (using emoji2vec embeddings), to find the emoji that has the highest cosine similarity score with the embedding of the predicted emotion label. This emoji will be outputted as the correct emoji. It's important to note that this approach requires a significant amount of data to train the model effectively, so selecting a high-quality dataset that is appropriate for the task is critical.\n",
    "\n",
    "### 6.3 Stretch Goals\n",
    "- (At least two goals here. These should be ambitious extensions to your desired goals. You can still get full points without completing these.)\n",
    "\n",
    "1. Emotion-specific emojis: Instead of simply mapping from predicted emotions to existing emojis, we want to be able to generate possibly new, emotion-specific emojis. This would involve training a generative model such as Variational Autoencoders (VAE) or Generative Adversarial Networks (GANs) on a large dataset of emojis, then using it to generate new emojis to correspond to specific emotions. \n",
    "\n",
    "2. Multi-modal input: Currently, our model is only based on text input. However, other modalities such as images or audio could be used as imput to improve the accuracy of the emoji detection. An idea could be to train a separate model to detect emotions in a images or audio and then combine those results with the text-based model to generate a more robust prediction. \n",
    "\n",
    "3. Different models: Our model is based on regular BERT. However, using slightly modified models such as RoBERTa, DistilBERT, or ALBERTA can be used to create models that could be slightly more accurate, faster, or improve fits to different corpora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlB_wLS381Xy"
   },
   "source": [
    "## 7. Hopes and Concerns\n",
    "\n",
    "*What are you most excited about with this project? What parts, if any, are you nervous about?\n",
    "\n",
    "We’re excited that we will be able to output emojis and classify very relevant type of comments that we see on the daily. We are nervous about being able to implement this model since it seems very complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2peFc_M8-E7"
   },
   "source": [
    "## 8. References\n",
    "\n",
    "*Cite the papers or sources that you used to discover your datasets and/or models, if you didn't include the citation above.*\n",
    "\n",
    "Demszky, Dorottya, et al. \"GoEmotions: A Dataset of Fine-Grained Emotions\" https://arxiv.org/pdf/2005.00547v2.pdf (2020).\n",
    "\n",
    "Shoeb, Abu, et al. \"EmoTag - Towards an Emotion-Based Analysis of Emojis\n",
    "\n",
    "Devlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n",
    "\n",
    "BERT documentation: https://huggingface.co/docs/transformers/model_doc/bert \n",
    "\n",
    "Fine-tuning BERT: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=unjuTtKUjZI3 \n",
    "\n",
    "How to use dataset: https://github.com/huggingface/datasets \n",
    "\n",
    "Emoji2Vec embeddings paper:https://arxiv.org/pdf/1609.08359v2.pdf\n",
    "\n",
    "GoEmotions dataset:\n",
    "https://huggingface.co/datasets/go_emotions\n",
    "\n",
    "EmoTag 1200 dataset: \n",
    "https://github.com/abushoeb/EmoTag/tree/master"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
