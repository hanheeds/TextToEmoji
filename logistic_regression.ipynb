{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb","timestamp":1684970766117}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Install the necessary modules"],"metadata":{"id":"MKmRZPc7twCg"}},{"cell_type":"code","metadata":{"id":"4wxY3x-ZZz8h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53419615-435c-4ac3-c9cd-2136a8835ef7"},"source":["!pip install -q transformers datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install datasets\n","#!pip install -q transformers datasets\n","#!pip install -q --upgrade transformers\n","#!pip install -q --upgrade datasets\n","!pip install -q accelerate\n","\n","# !pip uninstall -y transformers accelerate\n","# !pip install transformers accelerate\n","\n","!pip install -q --upgrade accelerate\n"],"metadata":{"id":"EiqZlyZpGUfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bIH9NP0MZ6-O"},"source":["## Load dataset\n","\n","Download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n","\n"]},{"cell_type":"code","metadata":{"id":"sd1LiXGjZ420"},"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCL02vQgxYTO"},"source":["The dataset contains 3 splits: one for training, one for validation and one for testing."]},{"cell_type":"code","metadata":{"id":"pRd1kXQZjYIY"},"source":["dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgS0wMWExcqP"},"source":["Example of the training split:\n","The dataset contains ID, tweet, and labeled emotions."]},{"cell_type":"code","metadata":{"id":"unjuTtKUjZI3"},"source":["example = dataset['train'][0]\n","example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DV0Rtetxgd4"},"source":["\n","Creating a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."]},{"cell_type":"code","metadata":{"id":"e5vZhQpvkE8s"},"source":["labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess data for logistic classification\n","\n","Since text is not a format that is easy to train a model with, we are transforming the strings into vectors using sklearn's TfidfVectorizer."],"metadata":{"id":"FeYbzLjhNVrt"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","\n","def preprocess_data(dataset, labels):\n","    X, Y = [], []\n","    for row in dataset:\n","        X.append(row['Tweet'])\n","        Y.append([1 if row[label] else 0 for label in labels])\n","    return X,Y\n","    \n","X_train, y_train = preprocess_data(dataset['train'], labels)\n","X_val, y_val = preprocess_data(dataset['validation'], labels)\n","X_test, y_test = preprocess_data(dataset['test'], labels)\n","\n","# Vectorize the tweets\n","X_train= vectorizer.fit_transform(X_train)\n","X_val= vectorizer.transform(X_val)\n","X_test = vectorizer.transform(X_test)\n"],"metadata":{"id":"PlL95ID2Orow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see below, the tweet and the emotion labels are represented as vectors."],"metadata":{"id":"8PGgiS9-pC-Z"}},{"cell_type":"code","source":["print(X_train[0])\n","print(y_train[0])"],"metadata":{"id":"KMePJa3iQbub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Logistic Classification model\n","\n","We need to use multi target classification because each tweet can correspond to multiple emotions. We are going to use sklearn's MultiOutputClassifier with Logistic Regression as the estimator."],"metadata":{"id":"t2FZJW90Sj1i"}},{"cell_type":"code","source":["from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = MultiOutputClassifier(LogisticRegression()).fit(X_train, y_train)\n"],"metadata":{"id":"tpgplFPHOQJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the model\n","Use the test dataset to calculate the accuracy of the trained Logistic Regression model. "],"metadata":{"id":"epZacpfDsc-n"}},{"cell_type":"code","source":["# Change the vector prediction to emotions\n","def vectorToEmotions(y, labels):\n","    return [id2label[idx] for idx, label in enumerate(y[i]) if label == 1.0]"],"metadata":{"id":"X4CVmLW-pkJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_preds = clf.predict(X_train[:5])\n","for i in range(len(y_preds)):\n","    print(vectorToEmotions(y_preds,labels ))\n","    print(vectorToEmotions(y_train[:5], labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3-4H-19TOj1","executionInfo":{"status":"ok","timestamp":1686261554906,"user_tz":300,"elapsed":19,"user":{"displayName":"Si Woo Park","userId":"07708869361558286923"}},"outputId":"e4332781-7419-41e0-c15f-e5cff61ecedb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['optimism']\n","['anticipation', 'optimism', 'trust']\n","['joy', 'optimism']\n","['joy', 'love', 'optimism']\n","['disgust', 'joy']\n","['anger', 'disgust', 'joy', 'optimism']\n","['joy', 'optimism']\n","['joy', 'optimism']\n","['disgust']\n","['anger', 'disgust']\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","def evaluateModel(model, x_test, y_test):\n","    Y_pred = model.predict(x_test)\n","\n","    accuracy = accuracy_score(y_test, Y_pred)\n","    precision = precision_score(y_test, Y_pred, average='micro')\n","    recall = recall_score(y_test, Y_pred, average='micro')\n","    roc_auc = roc_auc_score(y_test, Y_pred, average='micro')\n","    f1 = f1_score(y_test, Y_pred, average='micro')\n","\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"ROC AUC:\", roc_auc)\n","    print(\"Recall:\", recall)\n","    print(\"F1-Score:\", f1)\n","    print()\n","\n","print('Training Accuracy')\n","evaluateModel(clf, X_train, y_train)\n","\n","print('Validation Accuracy')\n","evaluateModel(clf, X_val, y_val)\n","\n","print('Testing Accuracy')\n","evaluateModel(clf, X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_39JflWWIvM","executionInfo":{"status":"ok","timestamp":1686261645888,"user_tz":300,"elapsed":935,"user":{"displayName":"Si Woo Park","userId":"07708869361558286923"}},"outputId":"8a96e230-8e4a-426a-8af6-6e80d028cd64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy\n","Accuracy: 0.2883884176659842\n","Precision: 0.9549605133267522\n","ROC AUC: 0.7380360555565011\n","Recall: 0.482240777666999\n","F1-Score: 0.6408578999668765\n","\n","Validation Accuracy\n","Accuracy: 0.1523702031602709\n","Precision: 0.8033519553072626\n","ROC AUC: 0.6547563217668522\n","Recall: 0.3327163350300787\n","F1-Score: 0.4705497382198953\n","\n","Testing Accuracy\n","Accuracy: 0.16814973918379872\n","Precision: 0.8029705971506517\n","ROC AUC: 0.6567032794243043\n","Recall: 0.3366374380480366\n","F1-Score: 0.47439111747851\n","\n"]}]}]}